{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de961466-1487-4e52-99ed-5140bc83dbbd",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c9d9732c-537f-4d85-ab1f-c56982259acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('/home/roobz/Jupyter/afib-detector/src/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1377ae0e-decc-416b-9d3b-aebe71e30a06",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.optim.lr_scheduler import StepLR, ReduceLROnPlateau\n",
    "\n",
    "from eda import load_label_map\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import optuna\n",
    "\n",
    "from model import load_train_test_datasets, Afib_CNN, epoch_train, epoch_test, save_model\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9eaf8c6-94f2-4260-997e-c7b04c5d122f",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning with Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "5e4fc5c5-c0fa-4f5c-ad62-ffa56c094c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = '../data/physionet/afdb/'\n",
    "window_size = 2500\n",
    "model_train_kwargs = {\n",
    "    'train_size': 6000,\n",
    "    'batch_size': 64,\n",
    "}\n",
    "model_test_kwargs = {\n",
    "    'test_size': 10000,\n",
    "    'test_batch_size': 1000\n",
    "}\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "train_dataset, test_dataset = load_train_test_datasets(\n",
    "    data_folder,\n",
    "    window_size,\n",
    "    model_train_kwargs['train_size'],\n",
    "    model_test_kwargs['test_size'],\n",
    "    random_seed = 42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "a8c18afb-b48e-4ab8-ba32-8b789f7d9918",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    gamma = trial.suggest_loguniform(\"gamma\", 1e-5, 1-1e-10)\n",
    "    lr = trial.suggest_loguniform(\"lr\", 1e-2, 1)\n",
    "    \n",
    "    model = define_model(trial)\n",
    "    optimizer = optim.Adadelta(model.parameters(), lr=lr)\n",
    "    scheduler = StepLR(optimizer, step_size=1, gamma=gamma)\n",
    "#     scheduler = ReduceLROnPlateau(optimizer, mode='min', patience = 10) \n",
    "\n",
    "    \n",
    "    epochs = 30\n",
    "    result = None\n",
    "    for epoch in range(1, epochs+1):\n",
    "        epoch_train(model, device, train_dataset, optimizer, epoch, **model_train_kwargs)\n",
    "        acc, avg_loss = epoch_test(model, device, test_dataset, **model_test_kwargs)\n",
    "        scheduler.step()\n",
    "        result = avg_loss\n",
    "        trial.report(result, epoch)\n",
    "        \n",
    "        # Handle pruning based on the intermediate value.\n",
    "        if trial.should_prune():\n",
    "            raise optuna.exceptions.TrialPruned()\n",
    "        \n",
    "    return result\n",
    "    \n",
    "\n",
    "def define_model(trial):\n",
    "    repeat_layers = trial.suggest_int('repeat_layers', 1, 4)\n",
    "    model = Afib_CNN(2500, 2, repeat_layers).to(device)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "46e344dd-953e-4bde-9206-956ffd9ff20d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-09-11 20:38:21,102]\u001b[0m A new study created in memory with name: no-name-8d4265ad-5967-4383-9d44-7e6a273200c5\u001b[0m\n",
      "\u001b[32m[I 2021-09-11 20:40:11,476]\u001b[0m Trial 0 finished with value: 0.7367783203125 and parameters: {'gamma': 0.00012772331369366135, 'lr': 0.05841112158537962, 'repeat_layers': 2}. Best is trial 0 with value: 0.7367783203125.\u001b[0m\n",
      "\u001b[32m[I 2021-09-11 20:42:00,839]\u001b[0m Trial 1 finished with value: 0.733902587890625 and parameters: {'gamma': 0.002149700307714176, 'lr': 0.028419777140572332, 'repeat_layers': 2}. Best is trial 1 with value: 0.733902587890625.\u001b[0m\n",
      "\u001b[32m[I 2021-09-11 20:43:57,472]\u001b[0m Trial 2 finished with value: 0.7428860473632812 and parameters: {'gamma': 0.011208694286999171, 'lr': 0.0395028801649447, 'repeat_layers': 4}. Best is trial 1 with value: 0.733902587890625.\u001b[0m\n",
      "\u001b[32m[I 2021-09-11 20:45:56,880]\u001b[0m Trial 3 finished with value: 0.6926365966796875 and parameters: {'gamma': 0.02464489667378558, 'lr': 0.010306410400941065, 'repeat_layers': 4}. Best is trial 3 with value: 0.6926365966796875.\u001b[0m\n",
      "\u001b[32m[I 2021-09-11 20:47:42,836]\u001b[0m Trial 4 finished with value: 0.8054122314453125 and parameters: {'gamma': 3.979932063301234e-05, 'lr': 0.08025087870000569, 'repeat_layers': 1}. Best is trial 3 with value: 0.6926365966796875.\u001b[0m\n",
      "\u001b[32m[I 2021-09-11 20:47:46,843]\u001b[0m Trial 5 pruned. \u001b[0m\n",
      "\u001b[32m[I 2021-09-11 20:49:32,273]\u001b[0m Trial 6 finished with value: 1.3435986328125 and parameters: {'gamma': 0.8068134208699546, 'lr': 0.06566261117730021, 'repeat_layers': 1}. Best is trial 3 with value: 0.6926365966796875.\u001b[0m\n",
      "\u001b[32m[I 2021-09-11 20:49:36,289]\u001b[0m Trial 7 pruned. \u001b[0m\n",
      "\u001b[32m[I 2021-09-11 20:49:40,152]\u001b[0m Trial 8 pruned. \u001b[0m\n",
      "\u001b[32m[I 2021-09-11 20:51:35,937]\u001b[0m Trial 9 finished with value: 0.69814208984375 and parameters: {'gamma': 0.13484430420968505, 'lr': 0.032575686594449905, 'repeat_layers': 3}. Best is trial 3 with value: 0.6926365966796875.\u001b[0m\n",
      "\u001b[32m[I 2021-09-11 20:51:39,828]\u001b[0m Trial 10 pruned. \u001b[0m\n",
      "\u001b[32m[I 2021-09-11 20:51:43,733]\u001b[0m Trial 11 pruned. \u001b[0m\n",
      "\u001b[32m[I 2021-09-11 20:51:47,487]\u001b[0m Trial 12 pruned. \u001b[0m\n",
      "\u001b[32m[I 2021-09-11 20:51:51,391]\u001b[0m Trial 13 pruned. \u001b[0m\n",
      "\u001b[32m[I 2021-09-11 20:53:41,915]\u001b[0m Trial 14 finished with value: 0.6983107299804687 and parameters: {'gamma': 0.008428618604951263, 'lr': 0.019438135179455023, 'repeat_layers': 2}. Best is trial 3 with value: 0.6926365966796875.\u001b[0m\n",
      "\u001b[32m[I 2021-09-11 20:53:45,972]\u001b[0m Trial 15 pruned. \u001b[0m\n",
      "\u001b[32m[I 2021-09-11 20:53:49,848]\u001b[0m Trial 16 pruned. \u001b[0m\n",
      "\u001b[32m[I 2021-09-11 20:53:53,866]\u001b[0m Trial 17 pruned. \u001b[0m\n",
      "\u001b[32m[I 2021-09-11 20:53:57,777]\u001b[0m Trial 18 pruned. \u001b[0m\n",
      "\u001b[32m[I 2021-09-11 20:54:01,537]\u001b[0m Trial 19 pruned. \u001b[0m\n",
      "\u001b[32m[I 2021-09-11 20:56:01,381]\u001b[0m Trial 20 finished with value: 0.6958890380859375 and parameters: {'gamma': 0.0013384022131399918, 'lr': 0.013497209190425756, 'repeat_layers': 4}. Best is trial 3 with value: 0.6926365966796875.\u001b[0m\n",
      "\u001b[32m[I 2021-09-11 20:58:00,859]\u001b[0m Trial 21 finished with value: 0.6878639526367187 and parameters: {'gamma': 0.0017554683090782786, 'lr': 0.015338414098196466, 'repeat_layers': 4}. Best is trial 21 with value: 0.6878639526367187.\u001b[0m\n",
      "\u001b[32m[I 2021-09-11 21:00:00,813]\u001b[0m Trial 22 finished with value: 0.6963670654296875 and parameters: {'gamma': 0.001540883118299302, 'lr': 0.014190920618666857, 'repeat_layers': 4}. Best is trial 21 with value: 0.6878639526367187.\u001b[0m\n",
      "\u001b[32m[I 2021-09-11 21:00:04,815]\u001b[0m Trial 23 pruned. \u001b[0m\n",
      "\u001b[32m[I 2021-09-11 21:00:08,851]\u001b[0m Trial 24 pruned. \u001b[0m\n",
      "\u001b[32m[I 2021-09-11 21:00:12,893]\u001b[0m Trial 25 pruned. \u001b[0m\n",
      "\u001b[32m[I 2021-09-11 21:00:16,925]\u001b[0m Trial 26 pruned. \u001b[0m\n",
      "\u001b[32m[I 2021-09-11 21:00:20,813]\u001b[0m Trial 27 pruned. \u001b[0m\n",
      "\u001b[32m[I 2021-09-11 21:02:20,666]\u001b[0m Trial 28 finished with value: 0.6914309692382813 and parameters: {'gamma': 0.0001578442920965335, 'lr': 0.010244449522831617, 'repeat_layers': 4}. Best is trial 21 with value: 0.6878639526367187.\u001b[0m\n",
      "\u001b[32m[I 2021-09-11 21:04:20,720]\u001b[0m Trial 29 finished with value: 0.6722985229492188 and parameters: {'gamma': 0.00013179127031159106, 'lr': 0.010266580930130503, 'repeat_layers': 4}. Best is trial 29 with value: 0.6722985229492188.\u001b[0m\n",
      "\u001b[32m[I 2021-09-11 21:06:16,434]\u001b[0m Trial 30 finished with value: 0.6270741577148438 and parameters: {'gamma': 1.6480207312276725e-05, 'lr': 0.047516406066688756, 'repeat_layers': 3}. Best is trial 30 with value: 0.6270741577148438.\u001b[0m\n",
      "\u001b[32m[I 2021-09-11 21:06:20,333]\u001b[0m Trial 31 pruned. \u001b[0m\n",
      "\u001b[32m[I 2021-09-11 21:06:24,376]\u001b[0m Trial 32 pruned. \u001b[0m\n",
      "\u001b[32m[I 2021-09-11 21:06:28,093]\u001b[0m Trial 33 pruned. \u001b[0m\n",
      "\u001b[32m[I 2021-09-11 21:06:32,136]\u001b[0m Trial 34 pruned. \u001b[0m\n",
      "\u001b[32m[I 2021-09-11 21:08:26,012]\u001b[0m Trial 35 finished with value: 0.6777587890625 and parameters: {'gamma': 0.0001226825767729461, 'lr': 0.040739808192301094, 'repeat_layers': 3}. Best is trial 30 with value: 0.6270741577148438.\u001b[0m\n",
      "\u001b[32m[I 2021-09-11 21:08:29,752]\u001b[0m Trial 36 pruned. \u001b[0m\n",
      "\u001b[32m[I 2021-09-11 21:10:14,974]\u001b[0m Trial 37 finished with value: 0.6724583740234376 and parameters: {'gamma': 0.00011031523005282614, 'lr': 0.07106853049080158, 'repeat_layers': 1}. Best is trial 30 with value: 0.6270741577148438.\u001b[0m\n",
      "\u001b[32m[I 2021-09-11 21:10:18,543]\u001b[0m Trial 38 pruned. \u001b[0m\n",
      "\u001b[32m[I 2021-09-11 21:10:22,092]\u001b[0m Trial 39 pruned. \u001b[0m\n",
      "\u001b[32m[I 2021-09-11 21:10:25,623]\u001b[0m Trial 40 pruned. \u001b[0m\n",
      "\u001b[32m[I 2021-09-11 21:12:20,492]\u001b[0m Trial 41 finished with value: 0.6882189331054688 and parameters: {'gamma': 0.00011013353683339562, 'lr': 0.03518881235604767, 'repeat_layers': 3}. Best is trial 30 with value: 0.6270741577148438.\u001b[0m\n",
      "\u001b[32m[I 2021-09-11 21:12:24,215]\u001b[0m Trial 42 pruned. \u001b[0m\n",
      "\u001b[32m[I 2021-09-11 21:12:28,109]\u001b[0m Trial 43 pruned. \u001b[0m\n",
      "\u001b[32m[I 2021-09-11 21:12:31,997]\u001b[0m Trial 44 pruned. \u001b[0m\n",
      "\u001b[32m[I 2021-09-11 21:12:35,499]\u001b[0m Trial 45 pruned. \u001b[0m\n",
      "\u001b[32m[I 2021-09-11 21:14:31,386]\u001b[0m Trial 46 finished with value: 0.659537109375 and parameters: {'gamma': 0.0005898163627794911, 'lr': 0.04020966942901233, 'repeat_layers': 3}. Best is trial 30 with value: 0.6270741577148438.\u001b[0m\n",
      "\u001b[32m[I 2021-09-11 21:14:35,286]\u001b[0m Trial 47 pruned. \u001b[0m\n",
      "\u001b[32m[I 2021-09-11 21:14:39,017]\u001b[0m Trial 48 pruned. \u001b[0m\n",
      "\u001b[32m[I 2021-09-11 21:14:42,913]\u001b[0m Trial 49 pruned. \u001b[0m\n",
      "\u001b[32m[I 2021-09-11 21:14:46,810]\u001b[0m Trial 50 pruned. \u001b[0m\n",
      "\u001b[32m[I 2021-09-11 21:14:50,703]\u001b[0m Trial 51 pruned. \u001b[0m\n",
      "\u001b[32m[I 2021-09-11 21:16:46,008]\u001b[0m Trial 52 finished with value: 0.68619140625 and parameters: {'gamma': 0.0008867389175176274, 'lr': 0.032655555178529076, 'repeat_layers': 3}. Best is trial 30 with value: 0.6270741577148438.\u001b[0m\n",
      "\u001b[32m[I 2021-09-11 21:16:49,896]\u001b[0m Trial 53 pruned. \u001b[0m\n",
      "\u001b[32m[I 2021-09-11 21:16:53,758]\u001b[0m Trial 54 pruned. \u001b[0m\n",
      "\u001b[32m[I 2021-09-11 21:16:57,494]\u001b[0m Trial 55 pruned. \u001b[0m\n",
      "\u001b[32m[I 2021-09-11 21:18:53,327]\u001b[0m Trial 56 finished with value: 0.6903878784179688 and parameters: {'gamma': 0.0002131946178939221, 'lr': 0.029612614252170617, 'repeat_layers': 3}. Best is trial 30 with value: 0.6270741577148438.\u001b[0m\n",
      "\u001b[32m[I 2021-09-11 21:18:57,224]\u001b[0m Trial 57 pruned. \u001b[0m\n",
      "\u001b[32m[I 2021-09-11 21:19:00,960]\u001b[0m Trial 58 pruned. \u001b[0m\n",
      "\u001b[32m[I 2021-09-11 21:19:04,709]\u001b[0m Trial 59 pruned. \u001b[0m\n",
      "\u001b[32m[I 2021-09-11 21:21:00,548]\u001b[0m Trial 60 finished with value: 0.664071044921875 and parameters: {'gamma': 0.00030365097969112286, 'lr': 0.024668033485652607, 'repeat_layers': 3}. Best is trial 30 with value: 0.6270741577148438.\u001b[0m\n",
      "\u001b[32m[I 2021-09-11 21:22:56,355]\u001b[0m Trial 61 finished with value: 0.6465401000976563 and parameters: {'gamma': 0.00039587044391934393, 'lr': 0.024442235541978368, 'repeat_layers': 3}. Best is trial 30 with value: 0.6270741577148438.\u001b[0m\n",
      "\u001b[32m[I 2021-09-11 21:23:00,251]\u001b[0m Trial 62 pruned. \u001b[0m\n",
      "\u001b[32m[I 2021-09-11 21:23:04,146]\u001b[0m Trial 63 pruned. \u001b[0m\n",
      "\u001b[32m[I 2021-09-11 21:25:00,077]\u001b[0m Trial 64 finished with value: 0.6716907348632812 and parameters: {'gamma': 0.0003161243705601889, 'lr': 0.040693046420097016, 'repeat_layers': 3}. Best is trial 30 with value: 0.6270741577148438.\u001b[0m\n",
      "\u001b[32m[I 2021-09-11 21:26:55,825]\u001b[0m Trial 65 finished with value: 0.6594490356445313 and parameters: {'gamma': 0.0003312675540865091, 'lr': 0.027608298386263525, 'repeat_layers': 3}. Best is trial 30 with value: 0.6270741577148438.\u001b[0m\n",
      "\u001b[32m[I 2021-09-11 21:26:59,708]\u001b[0m Trial 66 pruned. \u001b[0m\n",
      "\u001b[32m[I 2021-09-11 21:27:03,595]\u001b[0m Trial 67 pruned. \u001b[0m\n",
      "\u001b[32m[I 2021-09-11 21:27:07,475]\u001b[0m Trial 68 pruned. \u001b[0m\n",
      "\u001b[32m[I 2021-09-11 21:27:11,379]\u001b[0m Trial 69 pruned. \u001b[0m\n",
      "\u001b[32m[I 2021-09-11 21:27:15,426]\u001b[0m Trial 70 pruned. \u001b[0m\n",
      "\u001b[32m[I 2021-09-11 21:27:19,341]\u001b[0m Trial 71 pruned. \u001b[0m\n",
      "\u001b[32m[I 2021-09-11 21:27:23,240]\u001b[0m Trial 72 pruned. \u001b[0m\n",
      "\u001b[32m[I 2021-09-11 21:27:26,971]\u001b[0m Trial 73 pruned. \u001b[0m\n",
      "\u001b[32m[I 2021-09-11 21:27:30,869]\u001b[0m Trial 74 pruned. \u001b[0m\n",
      "\u001b[32m[I 2021-09-11 21:27:34,608]\u001b[0m Trial 75 pruned. \u001b[0m\n",
      "\u001b[32m[I 2021-09-11 21:27:38,506]\u001b[0m Trial 76 pruned. \u001b[0m\n",
      "\u001b[32m[I 2021-09-11 21:27:42,066]\u001b[0m Trial 77 pruned. \u001b[0m\n",
      "\u001b[32m[I 2021-09-11 21:27:46,064]\u001b[0m Trial 78 pruned. \u001b[0m\n",
      "\u001b[32m[I 2021-09-11 21:27:49,972]\u001b[0m Trial 79 pruned. \u001b[0m\n",
      "\u001b[32m[I 2021-09-11 21:27:53,997]\u001b[0m Trial 80 pruned. \u001b[0m\n",
      "\u001b[32m[I 2021-09-11 21:27:57,903]\u001b[0m Trial 81 pruned. \u001b[0m\n",
      "\u001b[32m[I 2021-09-11 21:29:53,376]\u001b[0m Trial 82 finished with value: 0.6717142944335938 and parameters: {'gamma': 0.00020009610870628532, 'lr': 0.039515319778324155, 'repeat_layers': 3}. Best is trial 30 with value: 0.6270741577148438.\u001b[0m\n",
      "\u001b[32m[I 2021-09-11 21:29:57,263]\u001b[0m Trial 83 pruned. \u001b[0m\n",
      "\u001b[32m[I 2021-09-11 21:30:01,121]\u001b[0m Trial 84 pruned. \u001b[0m\n",
      "\u001b[32m[I 2021-09-11 21:30:05,023]\u001b[0m Trial 85 pruned. \u001b[0m\n",
      "\u001b[32m[I 2021-09-11 21:30:08,910]\u001b[0m Trial 86 pruned. \u001b[0m\n",
      "\u001b[32m[I 2021-09-11 21:30:12,779]\u001b[0m Trial 87 pruned. \u001b[0m\n",
      "\u001b[32m[I 2021-09-11 21:30:16,700]\u001b[0m Trial 88 pruned. \u001b[0m\n",
      "\u001b[32m[I 2021-09-11 21:30:20,412]\u001b[0m Trial 89 pruned. \u001b[0m\n",
      "\u001b[32m[I 2021-09-11 21:30:24,466]\u001b[0m Trial 90 pruned. \u001b[0m\n",
      "\u001b[32m[I 2021-09-11 21:30:28,355]\u001b[0m Trial 91 pruned. \u001b[0m\n",
      "\u001b[32m[I 2021-09-11 21:30:32,257]\u001b[0m Trial 92 pruned. \u001b[0m\n",
      "\u001b[32m[I 2021-09-11 21:30:36,165]\u001b[0m Trial 93 pruned. \u001b[0m\n",
      "\u001b[32m[I 2021-09-11 21:30:40,055]\u001b[0m Trial 94 pruned. \u001b[0m\n",
      "\u001b[32m[I 2021-09-11 21:30:43,924]\u001b[0m Trial 95 pruned. \u001b[0m\n",
      "\u001b[32m[I 2021-09-11 21:30:47,829]\u001b[0m Trial 96 pruned. \u001b[0m\n",
      "\u001b[32m[I 2021-09-11 21:32:33,431]\u001b[0m Trial 97 finished with value: 0.6648558959960937 and parameters: {'gamma': 0.00022963899101960777, 'lr': 0.039782806527340134, 'repeat_layers': 1}. Best is trial 30 with value: 0.6270741577148438.\u001b[0m\n",
      "\u001b[32m[I 2021-09-11 21:34:18,682]\u001b[0m Trial 98 finished with value: 0.6602509155273437 and parameters: {'gamma': 0.0002725301322694487, 'lr': 0.05043973577009131, 'repeat_layers': 1}. Best is trial 30 with value: 0.6270741577148438.\u001b[0m\n",
      "\u001b[32m[I 2021-09-11 21:34:22,223]\u001b[0m Trial 99 pruned. \u001b[0m\n"
     ]
    }
   ],
   "source": [
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=100)\n",
    "\n",
    "pruned_trials = [t for t in study.trials if t.state == optuna.structs.TrialState.PRUNED]\n",
    "complete_trials = [t for t in study.trials if t.state == optuna.structs.TrialState.COMPLETE]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "4a4f06df-7b60-44f1-bd6c-febe5192fea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_train_kwargs = {\n",
    "    'train_size': 10000,\n",
    "    'batch_size': 64,\n",
    "}\n",
    "model_test_kwargs = {\n",
    "    'test_size': 10000,\n",
    "    'test_batch_size': 1000\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "7cc83e6f-f811-4053-93c1-2fe2de67b0c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model_args, epochs):\n",
    "    training_record = []\n",
    "    try:\n",
    "        model_args = pd.Series(model_args)\n",
    "        model = Afib_CNN(\n",
    "            input_size=2500,\n",
    "            channels=2,\n",
    "            repeat_layers=int(model_args.repeat_layers)\n",
    "        ).to(device)\n",
    "        optimizer = optim.Adadelta(model.parameters(), lr=model_args.lr)\n",
    "        scheduler = StepLR(optimizer, step_size=1, gamma=model_args.gamma)\n",
    "\n",
    "        print(f'Model size after convolutions: {model.fc1_size}')\n",
    "        for epoch in range(1, epochs+1):\n",
    "            \n",
    "            print(f'EPOCH {epoch}:\\tlr:{optimizer.param_groups[0][\"lr\"]: .3g}')\n",
    "            print(f'Training...', end='')\n",
    "            train_acc, train_loss = epoch_train(model, device, train_dataset, optimizer, epoch, **model_train_kwargs)\n",
    "            acc, loss = epoch_test(model, device, test_dataset, **model_test_kwargs)\n",
    "            scheduler.step()\n",
    "            print(f'\\rTrain Accuracy: {train_acc: .1f}% \\tTrain Loss: {train_loss: .4f}')\n",
    "            print(f'Test Accuracy:  {acc: .1f}% \\tTest Loss: {loss: .4f}')\n",
    "            \n",
    "            # record training\n",
    "            training_record.append((train_loss, loss))\n",
    "\n",
    "            \n",
    "\n",
    "        return model, training_record\n",
    "    except KeyboardInterrupt:\n",
    "        print('\\n-----Force Stop-----')\n",
    "        return model, training_record\n",
    "    \n",
    "def record_plot(record, figsize=None):\n",
    "    record = pd.DataFrame(record, columns=['Train', 'Test'])\n",
    "    ax = record.plot(xlabel='Epoch', ylabel='Loss', legend=True, figsize=figsize)\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "429eeb9a-c107-4bd3-bf89-2febc283a0e5",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Study statistics: \n",
      "  Number of finished trials:  100\n",
      "  Number of pruned trials:  73\n",
      "  Number of complete trials:  27\n",
      "Best trial:\n",
      "  Value:  0.6270741577148438\n",
      "  Params: \n",
      "    gamma: 1.6480207312276725e-05\n",
      "    lr: 0.047516406066688756\n",
      "    repeat_layers: 3\n",
      "Model size after convolutions: 27\n",
      "EPOCH 1:\tlr: 0.0475\n",
      "Train Accuracy:  76.6% \tTrain Loss:  0.5407\n",
      "Test Accuracy:   52.9% \tTest Loss:  0.7054\n",
      "EPOCH 2:\tlr: 7.83e-07\n",
      "Train Accuracy:  89.0% \tTrain Loss:  0.4216\n",
      "Test Accuracy:   53.3% \tTest Loss:  0.7093\n",
      "EPOCH 3:\tlr: 1.29e-11\n",
      "Train Accuracy:  89.6% \tTrain Loss:  0.4217\n",
      "Test Accuracy:   53.3% \tTest Loss:  0.7093\n",
      "EPOCH 4:\tlr: 2.13e-16\n",
      "Train Accuracy:  89.4% \tTrain Loss:  0.4216\n",
      "Test Accuracy:   53.3% \tTest Loss:  0.7093\n",
      "EPOCH 5:\tlr: 3.51e-21\n",
      "Train Accuracy:  89.4% \tTrain Loss:  0.4215\n",
      "Test Accuracy:   53.3% \tTest Loss:  0.7093\n",
      "EPOCH 6:\tlr: 5.78e-26\n",
      "Train Accuracy:  89.1% \tTrain Loss:  0.4243\n",
      "Test Accuracy:   53.3% \tTest Loss:  0.7093\n",
      "EPOCH 7:\tlr: 9.52e-31\n",
      "Train Accuracy:  88.3% \tTrain Loss:  0.4269\n",
      "Test Accuracy:   53.3% \tTest Loss:  0.7093\n",
      "EPOCH 8:\tlr: 1.57e-35\n",
      "Train Accuracy:  89.7% \tTrain Loss:  0.4185\n",
      "Test Accuracy:   53.3% \tTest Loss:  0.7093\n",
      "EPOCH 9:\tlr: 2.59e-40\n",
      "Train Accuracy:  89.2% \tTrain Loss:  0.4235\n",
      "Test Accuracy:   53.3% \tTest Loss:  0.7093\n",
      "EPOCH 10:\tlr: 4.26e-45\n",
      "Train Accuracy:  88.7% \tTrain Loss:  0.4264\n",
      "Test Accuracy:   53.3% \tTest Loss:  0.7093\n",
      "EPOCH 11:\tlr: 7.02e-50\n",
      "Train Accuracy:  89.5% \tTrain Loss:  0.4217\n",
      "Test Accuracy:   53.3% \tTest Loss:  0.7093\n",
      "EPOCH 12:\tlr: 1.16e-54\n",
      "Train Accuracy:  89.5% \tTrain Loss:  0.4221\n",
      "Test Accuracy:   53.3% \tTest Loss:  0.7093\n",
      "EPOCH 13:\tlr: 1.91e-59\n",
      "Train Accuracy:  89.5% \tTrain Loss:  0.4236\n",
      "Test Accuracy:   53.3% \tTest Loss:  0.7093\n",
      "EPOCH 14:\tlr: 3.14e-64\n",
      "Train Accuracy:  89.3% \tTrain Loss:  0.4236\n",
      "Test Accuracy:   53.3% \tTest Loss:  0.7093\n",
      "EPOCH 15:\tlr: 5.18e-69\n",
      "Train Accuracy:  89.0% \tTrain Loss:  0.4198\n",
      "Test Accuracy:   53.3% \tTest Loss:  0.7093\n",
      "EPOCH 16:\tlr: 8.54e-74\n",
      "Train Accuracy:  89.3% \tTrain Loss:  0.4242\n",
      "Test Accuracy:   53.3% \tTest Loss:  0.7093\n",
      "EPOCH 17:\tlr: 1.41e-78\n",
      "Train Accuracy:  89.4% \tTrain Loss:  0.4221\n",
      "Test Accuracy:   53.3% \tTest Loss:  0.7093\n",
      "EPOCH 18:\tlr: 2.32e-83\n",
      "Train Accuracy:  88.5% \tTrain Loss:  0.4236\n",
      "Test Accuracy:   53.3% \tTest Loss:  0.7093\n",
      "EPOCH 19:\tlr: 3.82e-88\n",
      "Train Accuracy:  89.2% \tTrain Loss:  0.4249\n",
      "Test Accuracy:   53.3% \tTest Loss:  0.7093\n",
      "EPOCH 20:\tlr: 6.3e-93\n",
      "Train Accuracy:  89.0% \tTrain Loss:  0.4229\n",
      "Test Accuracy:   53.3% \tTest Loss:  0.7093\n",
      "EPOCH 21:\tlr: 1.04e-97\n",
      "Train Accuracy:  89.5% \tTrain Loss:  0.4229\n",
      "Test Accuracy:   53.3% \tTest Loss:  0.7093\n",
      "EPOCH 22:\tlr: 1.71e-102\n",
      "Train Accuracy:  88.8% \tTrain Loss:  0.4244\n",
      "Test Accuracy:   53.3% \tTest Loss:  0.7093\n",
      "EPOCH 23:\tlr: 2.82e-107\n",
      "Train Accuracy:  89.7% \tTrain Loss:  0.4209\n",
      "Test Accuracy:   53.3% \tTest Loss:  0.7093\n",
      "EPOCH 24:\tlr: 4.64e-112\n",
      "Train Accuracy:  89.2% \tTrain Loss:  0.4223\n",
      "Test Accuracy:   53.3% \tTest Loss:  0.7093\n",
      "EPOCH 25:\tlr: 7.66e-117\n",
      "Train Accuracy:  88.9% \tTrain Loss:  0.4233\n",
      "Test Accuracy:   53.3% \tTest Loss:  0.7093\n",
      "EPOCH 26:\tlr: 1.26e-121\n",
      "Train Accuracy:  89.0% \tTrain Loss:  0.4233\n",
      "Test Accuracy:   53.3% \tTest Loss:  0.7093\n",
      "EPOCH 27:\tlr: 2.08e-126\n",
      "Train Accuracy:  89.0% \tTrain Loss:  0.4235\n",
      "Test Accuracy:   53.3% \tTest Loss:  0.7093\n",
      "EPOCH 28:\tlr: 3.43e-131\n",
      "Train Accuracy:  89.2% \tTrain Loss:  0.4210\n",
      "Test Accuracy:   53.3% \tTest Loss:  0.7093\n",
      "EPOCH 29:\tlr: 5.65e-136\n",
      "Train Accuracy:  89.2% \tTrain Loss:  0.4242\n",
      "Test Accuracy:   53.3% \tTest Loss:  0.7093\n",
      "EPOCH 30:\tlr: 9.31e-141\n",
      "Train Accuracy:  89.8% \tTrain Loss:  0.4217\n",
      "Test Accuracy:   53.3% \tTest Loss:  0.7093\n",
      "EPOCH 31:\tlr: 1.53e-145\n",
      "Train Accuracy:  88.8% \tTrain Loss:  0.4257\n",
      "Test Accuracy:   53.3% \tTest Loss:  0.7093\n",
      "EPOCH 32:\tlr: 2.53e-150\n",
      "Train Accuracy:  89.3% \tTrain Loss:  0.4212\n",
      "Test Accuracy:   53.3% \tTest Loss:  0.7093\n",
      "EPOCH 33:\tlr: 4.17e-155\n",
      "Train Accuracy:  89.0% \tTrain Loss:  0.4241\n",
      "Test Accuracy:   53.3% \tTest Loss:  0.7093\n",
      "EPOCH 34:\tlr: 6.86e-160\n",
      "Train Accuracy:  89.5% \tTrain Loss:  0.4234\n",
      "Test Accuracy:   53.3% \tTest Loss:  0.7093\n",
      "EPOCH 35:\tlr: 1.13e-164\n",
      "Train Accuracy:  89.2% \tTrain Loss:  0.4223\n",
      "Test Accuracy:   53.3% \tTest Loss:  0.7093\n",
      "EPOCH 36:\tlr: 1.86e-169\n",
      "Train Accuracy:  89.1% \tTrain Loss:  0.4264\n",
      "Test Accuracy:   53.3% \tTest Loss:  0.7093\n",
      "EPOCH 37:\tlr: 3.07e-174\n",
      "Train Accuracy:  89.3% \tTrain Loss:  0.4241\n",
      "Test Accuracy:   53.3% \tTest Loss:  0.7093\n",
      "EPOCH 38:\tlr: 5.06e-179\n",
      "Train Accuracy:  89.4% \tTrain Loss:  0.4238\n",
      "Test Accuracy:   53.3% \tTest Loss:  0.7093\n",
      "EPOCH 39:\tlr: 8.34e-184\n",
      "Train Accuracy:  89.1% \tTrain Loss:  0.4285\n",
      "Test Accuracy:   53.3% \tTest Loss:  0.7093\n",
      "EPOCH 40:\tlr: 1.38e-188\n",
      "Train Accuracy:  89.2% \tTrain Loss:  0.4248\n",
      "Test Accuracy:   53.3% \tTest Loss:  0.7093\n",
      "EPOCH 41:\tlr: 2.27e-193\n",
      "Train Accuracy:  89.5% \tTrain Loss:  0.4198\n",
      "Test Accuracy:   53.3% \tTest Loss:  0.7093\n",
      "EPOCH 42:\tlr: 3.74e-198\n",
      "Train Accuracy:  89.0% \tTrain Loss:  0.4230\n",
      "Test Accuracy:   53.3% \tTest Loss:  0.7093\n",
      "EPOCH 43:\tlr: 6.16e-203\n",
      "Train Accuracy:  89.1% \tTrain Loss:  0.4227\n",
      "Test Accuracy:   53.3% \tTest Loss:  0.7093\n",
      "EPOCH 44:\tlr: 1.01e-207\n",
      "Train Accuracy:  88.8% \tTrain Loss:  0.4232\n",
      "Test Accuracy:   53.3% \tTest Loss:  0.7093\n",
      "EPOCH 45:\tlr: 1.67e-212\n",
      "Train Accuracy:  88.9% \tTrain Loss:  0.4236\n",
      "Test Accuracy:   53.3% \tTest Loss:  0.7093\n",
      "EPOCH 46:\tlr: 2.76e-217\n",
      "Train Accuracy:  89.2% \tTrain Loss:  0.4206\n",
      "Test Accuracy:   53.3% \tTest Loss:  0.7093\n",
      "EPOCH 47:\tlr: 4.54e-222\n",
      "Train Accuracy:  89.2% \tTrain Loss:  0.4250\n",
      "Test Accuracy:   53.3% \tTest Loss:  0.7093\n",
      "EPOCH 48:\tlr: 7.48e-227\n",
      "Train Accuracy:  89.8% \tTrain Loss:  0.4230\n",
      "Test Accuracy:   53.3% \tTest Loss:  0.7093\n",
      "EPOCH 49:\tlr: 1.23e-231\n",
      "Train Accuracy:  89.4% \tTrain Loss:  0.4230\n",
      "Test Accuracy:   53.3% \tTest Loss:  0.7093\n",
      "EPOCH 50:\tlr: 2.03e-236\n",
      "Train Accuracy:  89.1% \tTrain Loss:  0.4217\n",
      "Test Accuracy:   53.3% \tTest Loss:  0.7093\n",
      "EPOCH 51:\tlr: 3.35e-241\n",
      "Train Accuracy:  89.2% \tTrain Loss:  0.4249\n",
      "Test Accuracy:   53.3% \tTest Loss:  0.7093\n",
      "EPOCH 52:\tlr: 5.52e-246\n",
      "Train Accuracy:  89.4% \tTrain Loss:  0.4207\n",
      "Test Accuracy:   53.3% \tTest Loss:  0.7093\n",
      "EPOCH 53:\tlr: 9.1e-251\n",
      "Train Accuracy:  89.3% \tTrain Loss:  0.4213\n",
      "Test Accuracy:   53.3% \tTest Loss:  0.7093\n",
      "EPOCH 54:\tlr: 1.5e-255\n",
      "Train Accuracy:  89.6% \tTrain Loss:  0.4210\n",
      "Test Accuracy:   53.3% \tTest Loss:  0.7093\n",
      "EPOCH 55:\tlr: 2.47e-260\n",
      "Train Accuracy:  89.6% \tTrain Loss:  0.4220\n",
      "Test Accuracy:   53.3% \tTest Loss:  0.7093\n",
      "EPOCH 56:\tlr: 4.07e-265\n",
      "Train Accuracy:  89.2% \tTrain Loss:  0.4242\n",
      "Test Accuracy:   53.3% \tTest Loss:  0.7093\n",
      "EPOCH 57:\tlr: 6.71e-270\n",
      "Train Accuracy:  88.8% \tTrain Loss:  0.4253\n",
      "Test Accuracy:   53.3% \tTest Loss:  0.7093\n",
      "EPOCH 58:\tlr: 1.11e-274\n",
      "Train Accuracy:  89.6% \tTrain Loss:  0.4220\n",
      "Test Accuracy:   53.3% \tTest Loss:  0.7093\n",
      "EPOCH 59:\tlr: 1.82e-279\n",
      "Train Accuracy:  89.1% \tTrain Loss:  0.4204\n",
      "Test Accuracy:   53.3% \tTest Loss:  0.7093\n",
      "EPOCH 60:\tlr: 3e-284\n",
      "Train Accuracy:  89.2% \tTrain Loss:  0.4246\n",
      "Test Accuracy:   53.3% \tTest Loss:  0.7093\n",
      "EPOCH 61:\tlr: 4.95e-289\n",
      "Train Accuracy:  89.2% \tTrain Loss:  0.4235\n",
      "Test Accuracy:   53.3% \tTest Loss:  0.7093\n",
      "EPOCH 62:\tlr: 8.16e-294\n",
      "Train Accuracy:  89.2% \tTrain Loss:  0.4226\n",
      "Test Accuracy:   53.3% \tTest Loss:  0.7093\n",
      "EPOCH 63:\tlr: 1.34e-298\n",
      "Training...\n",
      "-----Force Stop-----\n"
     ]
    }
   ],
   "source": [
    "print(\"Study statistics: \")\n",
    "print(\"  Number of finished trials: \", len(study.trials))\n",
    "print(\"  Number of pruned trials: \", len(pruned_trials))\n",
    "print(\"  Number of complete trials: \", len(complete_trials))\n",
    "\n",
    "print(\"Best trial:\")\n",
    "best_trial = study.best_trial\n",
    "\n",
    "print(\"  Value: \", best_trial.value)\n",
    "\n",
    "print(\"  Params: \")\n",
    "for key, value in best_trial.params.items():\n",
    "    print(\"    {}: {}\".format(key, value))\n",
    "\n",
    "    \n",
    "model, training_record = train_model(\n",
    "    study.best_params, \n",
    "    epochs=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "ade52614-e362-4859-8fde-04746d7b83f6",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model size after convolutions: 91\n",
      "EPOCH 1:\tlr: 0.01\n",
      "Train Accuracy:  57.2% \tTrain Loss:  0.6825\n",
      "Test Accuracy:   58.8% \tTest Loss:  0.6750\n",
      "EPOCH 2:\tlr: 0.0075\n",
      "Train Accuracy:  68.0% \tTrain Loss:  0.6164\n",
      "Test Accuracy:   59.2% \tTest Loss:  0.6767\n",
      "EPOCH 3:\tlr: 0.00562\n",
      "Train Accuracy:  73.5% \tTrain Loss:  0.5719\n",
      "Test Accuracy:   59.3% \tTest Loss:  0.6772\n",
      "EPOCH 4:\tlr: 0.00422\n",
      "Train Accuracy:  77.5% \tTrain Loss:  0.5382\n",
      "Test Accuracy:   59.6% \tTest Loss:  0.6765\n",
      "EPOCH 5:\tlr: 0.00316\n",
      "Train Accuracy:  79.1% \tTrain Loss:  0.5127\n",
      "Test Accuracy:   60.1% \tTest Loss:  0.6746\n",
      "EPOCH 6:\tlr: 0.00237\n",
      "Train Accuracy:  81.7% \tTrain Loss:  0.4907\n",
      "Test Accuracy:   60.4% \tTest Loss:  0.6734\n",
      "EPOCH 7:\tlr: 0.00178\n",
      "Train Accuracy:  82.3% \tTrain Loss:  0.4787\n",
      "Test Accuracy:   60.1% \tTest Loss:  0.6719\n",
      "EPOCH 8:\tlr: 0.00133\n",
      "Train Accuracy:  83.4% \tTrain Loss:  0.4659\n",
      "Test Accuracy:   60.2% \tTest Loss:  0.6722\n",
      "EPOCH 9:\tlr: 0.001\n",
      "Train Accuracy:  84.1% \tTrain Loss:  0.4563\n",
      "Test Accuracy:   59.8% \tTest Loss:  0.6721\n",
      "EPOCH 10:\tlr: 0.000751\n",
      "Train Accuracy:  84.2% \tTrain Loss:  0.4529\n",
      "Test Accuracy:   59.8% \tTest Loss:  0.6720\n",
      "EPOCH 11:\tlr: 0.000563\n",
      "Train Accuracy:  85.2% \tTrain Loss:  0.4463\n",
      "Test Accuracy:   59.7% \tTest Loss:  0.6721\n",
      "EPOCH 12:\tlr: 0.000422\n",
      "Train Accuracy:  85.0% \tTrain Loss:  0.4444\n",
      "Test Accuracy:   59.7% \tTest Loss:  0.6720\n",
      "EPOCH 13:\tlr: 0.000317\n",
      "Train Accuracy:  85.9% \tTrain Loss:  0.4357\n",
      "Test Accuracy:   60.0% \tTest Loss:  0.6721\n",
      "EPOCH 14:\tlr: 0.000238\n",
      "Train Accuracy:  85.2% \tTrain Loss:  0.4388\n",
      "Test Accuracy:   60.0% \tTest Loss:  0.6721\n",
      "EPOCH 15:\tlr: 0.000178\n",
      "Train Accuracy:  85.8% \tTrain Loss:  0.4359\n",
      "Test Accuracy:   60.0% \tTest Loss:  0.6721\n",
      "EPOCH 16:\tlr: 0.000134\n",
      "Train Accuracy:  85.6% \tTrain Loss:  0.4357\n",
      "Test Accuracy:   60.0% \tTest Loss:  0.6721\n",
      "EPOCH 17:\tlr: 0.0001\n",
      "Train Accuracy:  85.8% \tTrain Loss:  0.4372\n",
      "Test Accuracy:   60.1% \tTest Loss:  0.6722\n",
      "EPOCH 18:\tlr: 7.52e-05\n",
      "Train Accuracy:  85.6% \tTrain Loss:  0.4353\n",
      "Test Accuracy:   60.1% \tTest Loss:  0.6722\n",
      "EPOCH 19:\tlr: 5.64e-05\n",
      "Train Accuracy:  85.4% \tTrain Loss:  0.4354\n",
      "Test Accuracy:   60.2% \tTest Loss:  0.6722\n",
      "EPOCH 20:\tlr: 4.23e-05\n",
      "Train Accuracy:  86.3% \tTrain Loss:  0.4315\n",
      "Test Accuracy:   60.2% \tTest Loss:  0.6722\n",
      "EPOCH 21:\tlr: 3.17e-05\n",
      "Train Accuracy:  86.2% \tTrain Loss:  0.4351\n",
      "Test Accuracy:   60.3% \tTest Loss:  0.6722\n",
      "EPOCH 22:\tlr: 2.38e-05\n",
      "Train Accuracy:  86.1% \tTrain Loss:  0.4353\n",
      "Test Accuracy:   60.3% \tTest Loss:  0.6722\n",
      "EPOCH 23:\tlr: 1.78e-05\n",
      "Train Accuracy:  86.1% \tTrain Loss:  0.4330\n",
      "Test Accuracy:   60.3% \tTest Loss:  0.6722\n",
      "EPOCH 24:\tlr: 1.34e-05\n",
      "Train Accuracy:  85.9% \tTrain Loss:  0.4345\n",
      "Test Accuracy:   60.3% \tTest Loss:  0.6722\n",
      "EPOCH 25:\tlr: 1e-05\n",
      "Train Accuracy:  85.4% \tTrain Loss:  0.4342\n",
      "Test Accuracy:   60.3% \tTest Loss:  0.6722\n",
      "EPOCH 26:\tlr: 7.53e-06\n",
      "Train Accuracy:  86.0% \tTrain Loss:  0.4323\n",
      "Test Accuracy:   60.3% \tTest Loss:  0.6722\n",
      "EPOCH 27:\tlr: 5.64e-06\n",
      "Train Accuracy:  86.6% \tTrain Loss:  0.4316\n",
      "Test Accuracy:   60.3% \tTest Loss:  0.6722\n",
      "EPOCH 28:\tlr: 4.23e-06\n",
      "Train Accuracy:  86.1% \tTrain Loss:  0.4312\n",
      "Test Accuracy:   60.3% \tTest Loss:  0.6722\n",
      "EPOCH 29:\tlr: 3.17e-06\n",
      "Train Accuracy:  86.0% \tTrain Loss:  0.4331\n",
      "Test Accuracy:   60.3% \tTest Loss:  0.6722\n",
      "EPOCH 30:\tlr: 2.38e-06\n",
      "Train Accuracy:  85.8% \tTrain Loss:  0.4327\n",
      "Test Accuracy:   60.3% \tTest Loss:  0.6722\n",
      "EPOCH 31:\tlr: 1.79e-06\n",
      "Train Accuracy:  85.9% \tTrain Loss:  0.4335\n",
      "Test Accuracy:   60.3% \tTest Loss:  0.6723\n",
      "EPOCH 32:\tlr: 1.34e-06\n",
      "Train Accuracy:  86.0% \tTrain Loss:  0.4324\n",
      "Test Accuracy:   60.3% \tTest Loss:  0.6723\n",
      "EPOCH 33:\tlr: 1e-06\n",
      "Train Accuracy:  86.0% \tTrain Loss:  0.4328\n",
      "Test Accuracy:   60.3% \tTest Loss:  0.6723\n",
      "EPOCH 34:\tlr: 7.53e-07\n",
      "Train Accuracy:  86.5% \tTrain Loss:  0.4314\n",
      "Test Accuracy:   60.3% \tTest Loss:  0.6723\n",
      "EPOCH 35:\tlr: 5.65e-07\n",
      "Train Accuracy:  86.1% \tTrain Loss:  0.4313\n",
      "Test Accuracy:   60.3% \tTest Loss:  0.6723\n",
      "EPOCH 36:\tlr: 4.24e-07\n",
      "Train Accuracy:  85.9% \tTrain Loss:  0.4331\n",
      "Test Accuracy:   60.3% \tTest Loss:  0.6723\n",
      "EPOCH 37:\tlr: 3.18e-07\n",
      "Train Accuracy:  85.7% \tTrain Loss:  0.4320\n",
      "Test Accuracy:   60.3% \tTest Loss:  0.6723\n",
      "EPOCH 38:\tlr: 2.38e-07\n",
      "Train Accuracy:  86.1% \tTrain Loss:  0.4324\n",
      "Test Accuracy:   60.3% \tTest Loss:  0.6723\n",
      "EPOCH 39:\tlr: 1.79e-07\n",
      "Train Accuracy:  86.3% \tTrain Loss:  0.4301\n",
      "Test Accuracy:   60.3% \tTest Loss:  0.6723\n",
      "EPOCH 40:\tlr: 1.34e-07\n",
      "Train Accuracy:  85.5% \tTrain Loss:  0.4329\n",
      "Test Accuracy:   60.3% \tTest Loss:  0.6723\n",
      "EPOCH 41:\tlr: 1.01e-07\n",
      "Train Accuracy:  86.0% \tTrain Loss:  0.4318\n",
      "Test Accuracy:   60.3% \tTest Loss:  0.6723\n",
      "EPOCH 42:\tlr: 7.54e-08\n",
      "Train Accuracy:  85.9% \tTrain Loss:  0.4320\n",
      "Test Accuracy:   60.3% \tTest Loss:  0.6723\n",
      "EPOCH 43:\tlr: 5.66e-08\n",
      "Train Accuracy:  86.0% \tTrain Loss:  0.4330\n",
      "Test Accuracy:   60.3% \tTest Loss:  0.6723\n",
      "EPOCH 44:\tlr: 4.24e-08\n",
      "Train Accuracy:  85.9% \tTrain Loss:  0.4317\n",
      "Test Accuracy:   60.3% \tTest Loss:  0.6723\n",
      "EPOCH 45:\tlr: 3.18e-08\n",
      "Train Accuracy:  85.9% \tTrain Loss:  0.4323\n",
      "Test Accuracy:   60.3% \tTest Loss:  0.6723\n",
      "EPOCH 46:\tlr: 2.39e-08\n",
      "Train Accuracy:  86.5% \tTrain Loss:  0.4321\n",
      "Test Accuracy:   60.3% \tTest Loss:  0.6723\n",
      "EPOCH 47:\tlr: 1.79e-08\n",
      "Train Accuracy:  85.6% \tTrain Loss:  0.4346\n",
      "Test Accuracy:   60.3% \tTest Loss:  0.6723\n",
      "EPOCH 48:\tlr: 1.34e-08\n",
      "Train Accuracy:  85.9% \tTrain Loss:  0.4338\n",
      "Test Accuracy:   60.3% \tTest Loss:  0.6723\n",
      "EPOCH 49:\tlr: 1.01e-08\n",
      "Train Accuracy:  86.7% \tTrain Loss:  0.4300\n",
      "Test Accuracy:   60.3% \tTest Loss:  0.6723\n",
      "EPOCH 50:\tlr: 7.55e-09\n",
      "Train Accuracy:  86.2% \tTrain Loss:  0.4324\n",
      "Test Accuracy:   60.3% \tTest Loss:  0.6723\n",
      "EPOCH 51:\tlr: 5.66e-09\n",
      "Train Accuracy:  86.2% \tTrain Loss:  0.4328\n",
      "Test Accuracy:   60.3% \tTest Loss:  0.6723\n",
      "EPOCH 52:\tlr: 4.25e-09\n",
      "Train Accuracy:  86.0% \tTrain Loss:  0.4340\n",
      "Test Accuracy:   60.3% \tTest Loss:  0.6723\n",
      "EPOCH 53:\tlr: 3.19e-09\n",
      "Train Accuracy:  85.8% \tTrain Loss:  0.4316\n",
      "Test Accuracy:   60.3% \tTest Loss:  0.6723\n",
      "EPOCH 54:\tlr: 2.39e-09\n",
      "Train Accuracy:  86.0% \tTrain Loss:  0.4304\n",
      "Test Accuracy:   60.3% \tTest Loss:  0.6723\n",
      "EPOCH 55:\tlr: 1.79e-09\n",
      "Train Accuracy:  86.2% \tTrain Loss:  0.4299\n",
      "Test Accuracy:   60.3% \tTest Loss:  0.6723\n",
      "EPOCH 56:\tlr: 1.34e-09\n",
      "Train Accuracy:  86.3% \tTrain Loss:  0.4313\n",
      "Test Accuracy:   60.3% \tTest Loss:  0.6723\n",
      "EPOCH 57:\tlr: 1.01e-09\n",
      "Train Accuracy:  85.9% \tTrain Loss:  0.4340\n",
      "Test Accuracy:   60.3% \tTest Loss:  0.6723\n",
      "EPOCH 58:\tlr: 7.56e-10\n",
      "Train Accuracy:  86.0% \tTrain Loss:  0.4318\n",
      "Test Accuracy:   60.3% \tTest Loss:  0.6723\n",
      "EPOCH 59:\tlr: 5.67e-10\n",
      "Train Accuracy:  85.6% \tTrain Loss:  0.4346\n",
      "Test Accuracy:   60.3% \tTest Loss:  0.6723\n",
      "EPOCH 60:\tlr: 4.25e-10\n",
      "Train Accuracy:  85.8% \tTrain Loss:  0.4307\n",
      "Test Accuracy:   60.3% \tTest Loss:  0.6723\n",
      "EPOCH 61:\tlr: 3.19e-10\n",
      "Train Accuracy:  85.6% \tTrain Loss:  0.4337\n",
      "Test Accuracy:   60.3% \tTest Loss:  0.6723\n",
      "EPOCH 62:\tlr: 2.39e-10\n",
      "Train Accuracy:  86.0% \tTrain Loss:  0.4337\n",
      "Test Accuracy:   60.3% \tTest Loss:  0.6723\n",
      "EPOCH 63:\tlr: 1.79e-10\n",
      "Train Accuracy:  85.9% \tTrain Loss:  0.4339\n",
      "Test Accuracy:   60.3% \tTest Loss:  0.6723\n",
      "EPOCH 64:\tlr: 1.35e-10\n",
      "Train Accuracy:  85.9% \tTrain Loss:  0.4322\n",
      "Test Accuracy:   60.3% \tTest Loss:  0.6723\n",
      "EPOCH 65:\tlr: 1.01e-10\n",
      "Train Accuracy:  85.8% \tTrain Loss:  0.4331\n",
      "Test Accuracy:   60.3% \tTest Loss:  0.6723\n",
      "EPOCH 66:\tlr: 7.57e-11\n",
      "Train Accuracy:  86.3% \tTrain Loss:  0.4327\n",
      "Test Accuracy:   60.3% \tTest Loss:  0.6723\n",
      "EPOCH 67:\tlr: 5.68e-11\n",
      "Train Accuracy:  85.6% \tTrain Loss:  0.4336\n",
      "Test Accuracy:   60.3% \tTest Loss:  0.6723\n",
      "EPOCH 68:\tlr: 4.26e-11\n",
      "Train Accuracy:  86.3% \tTrain Loss:  0.4313\n",
      "Test Accuracy:   60.3% \tTest Loss:  0.6723\n",
      "EPOCH 69:\tlr: 3.19e-11\n",
      "Train Accuracy:  86.2% \tTrain Loss:  0.4320\n",
      "Test Accuracy:   60.3% \tTest Loss:  0.6723\n",
      "EPOCH 70:\tlr: 2.39e-11\n",
      "Train Accuracy:  85.6% \tTrain Loss:  0.4309\n",
      "Test Accuracy:   60.3% \tTest Loss:  0.6723\n",
      "EPOCH 71:\tlr: 1.8e-11\n",
      "Train Accuracy:  86.3% \tTrain Loss:  0.4302\n",
      "Test Accuracy:   60.3% \tTest Loss:  0.6723\n",
      "EPOCH 72:\tlr: 1.35e-11\n",
      "Train Accuracy:  85.9% \tTrain Loss:  0.4329\n",
      "Test Accuracy:   60.3% \tTest Loss:  0.6723\n",
      "EPOCH 73:\tlr: 1.01e-11\n",
      "Train Accuracy:  85.8% \tTrain Loss:  0.4331\n",
      "Test Accuracy:   60.3% \tTest Loss:  0.6723\n",
      "EPOCH 74:\tlr: 7.58e-12\n",
      "Train Accuracy:  85.9% \tTrain Loss:  0.4316\n",
      "Test Accuracy:   60.3% \tTest Loss:  0.6723\n",
      "EPOCH 75:\tlr: 5.68e-12\n",
      "Train Accuracy:  86.0% \tTrain Loss:  0.4312\n",
      "Test Accuracy:   60.3% \tTest Loss:  0.6723\n",
      "EPOCH 76:\tlr: 4.26e-12\n",
      "Train Accuracy:  85.2% \tTrain Loss:  0.4351\n",
      "Test Accuracy:   60.3% \tTest Loss:  0.6723\n",
      "EPOCH 77:\tlr: 3.2e-12\n",
      "Train Accuracy:  85.9% \tTrain Loss:  0.4327\n",
      "Test Accuracy:   60.3% \tTest Loss:  0.6723\n",
      "EPOCH 78:\tlr: 2.4e-12\n",
      "Train Accuracy:  86.0% \tTrain Loss:  0.4333\n",
      "Test Accuracy:   60.3% \tTest Loss:  0.6723\n",
      "EPOCH 79:\tlr: 1.8e-12\n",
      "Train Accuracy:  85.6% \tTrain Loss:  0.4336\n",
      "Test Accuracy:   60.3% \tTest Loss:  0.6723\n",
      "EPOCH 80:\tlr: 1.35e-12\n",
      "Train Accuracy:  85.5% \tTrain Loss:  0.4336\n",
      "Test Accuracy:   60.3% \tTest Loss:  0.6723\n",
      "EPOCH 81:\tlr: 1.01e-12\n",
      "Train Accuracy:  85.5% \tTrain Loss:  0.4348\n",
      "Test Accuracy:   60.3% \tTest Loss:  0.6723\n",
      "EPOCH 82:\tlr: 7.59e-13\n",
      "Train Accuracy:  85.9% \tTrain Loss:  0.4327\n",
      "Test Accuracy:   60.3% \tTest Loss:  0.6723\n",
      "EPOCH 83:\tlr: 5.69e-13\n",
      "Train Accuracy:  85.7% \tTrain Loss:  0.4328\n",
      "Test Accuracy:   60.3% \tTest Loss:  0.6723\n",
      "EPOCH 84:\tlr: 4.27e-13\n",
      "Train Accuracy:  86.4% \tTrain Loss:  0.4279\n",
      "Test Accuracy:   60.3% \tTest Loss:  0.6723\n",
      "EPOCH 85:\tlr: 3.2e-13\n",
      "Train Accuracy:  85.9% \tTrain Loss:  0.4329\n",
      "Test Accuracy:   60.3% \tTest Loss:  0.6723\n",
      "EPOCH 86:\tlr: 2.4e-13\n",
      "Train Accuracy:  85.8% \tTrain Loss:  0.4336\n",
      "Test Accuracy:   60.3% \tTest Loss:  0.6723\n",
      "EPOCH 87:\tlr: 1.8e-13\n",
      "Train Accuracy:  85.7% \tTrain Loss:  0.4320\n",
      "Test Accuracy:   60.3% \tTest Loss:  0.6723\n",
      "EPOCH 88:\tlr: 1.35e-13\n",
      "Train Accuracy:  86.2% \tTrain Loss:  0.4332\n",
      "Test Accuracy:   60.3% \tTest Loss:  0.6723\n",
      "EPOCH 89:\tlr: 1.01e-13\n",
      "Train Accuracy:  85.6% \tTrain Loss:  0.4335\n",
      "Test Accuracy:   60.3% \tTest Loss:  0.6723\n",
      "EPOCH 90:\tlr: 7.59e-14\n",
      "Train Accuracy:  85.4% \tTrain Loss:  0.4340\n",
      "Test Accuracy:   60.3% \tTest Loss:  0.6723\n",
      "EPOCH 91:\tlr: 5.7e-14\n",
      "Train Accuracy:  86.2% \tTrain Loss:  0.4312\n",
      "Test Accuracy:   60.3% \tTest Loss:  0.6723\n",
      "EPOCH 92:\tlr: 4.27e-14\n",
      "Train Accuracy:  85.8% \tTrain Loss:  0.4329\n",
      "Test Accuracy:   60.3% \tTest Loss:  0.6723\n",
      "EPOCH 93:\tlr: 3.2e-14\n",
      "Train Accuracy:  85.4% \tTrain Loss:  0.4355\n",
      "Test Accuracy:   60.3% \tTest Loss:  0.6723\n",
      "EPOCH 94:\tlr: 2.4e-14\n",
      "Train Accuracy:  86.3% \tTrain Loss:  0.4310\n",
      "Test Accuracy:   60.3% \tTest Loss:  0.6723\n",
      "EPOCH 95:\tlr: 1.8e-14\n",
      "Train Accuracy:  86.1% \tTrain Loss:  0.4319\n",
      "Test Accuracy:   60.3% \tTest Loss:  0.6723\n",
      "EPOCH 96:\tlr: 1.35e-14\n",
      "Train Accuracy:  86.0% \tTrain Loss:  0.4313\n",
      "Test Accuracy:   60.3% \tTest Loss:  0.6723\n",
      "EPOCH 97:\tlr: 1.01e-14\n",
      "Train Accuracy:  86.1% \tTrain Loss:  0.4314\n",
      "Test Accuracy:   60.3% \tTest Loss:  0.6723\n",
      "EPOCH 98:\tlr: 7.6e-15\n",
      "Train Accuracy:  86.1% \tTrain Loss:  0.4349\n",
      "Test Accuracy:   60.3% \tTest Loss:  0.6723\n",
      "EPOCH 99:\tlr: 5.7e-15\n",
      "Train Accuracy:  85.8% \tTrain Loss:  0.4320\n",
      "Test Accuracy:   60.3% \tTest Loss:  0.6723\n",
      "EPOCH 100:\tlr: 4.28e-15\n",
      "Train Accuracy:  86.0% \tTrain Loss:  0.4335\n",
      "Test Accuracy:   60.3% \tTest Loss:  0.6723\n",
      "EPOCH 101:\tlr: 3.21e-15\n",
      "Train Accuracy:  86.0% \tTrain Loss:  0.4319\n",
      "Test Accuracy:   60.3% \tTest Loss:  0.6723\n",
      "EPOCH 102:\tlr: 2.41e-15\n",
      "Train Accuracy:  86.5% \tTrain Loss:  0.4303\n",
      "Test Accuracy:   60.3% \tTest Loss:  0.6723\n",
      "EPOCH 103:\tlr: 1.8e-15\n",
      "Train Accuracy:  86.2% \tTrain Loss:  0.4317\n",
      "Test Accuracy:   60.3% \tTest Loss:  0.6723\n",
      "EPOCH 104:\tlr: 1.35e-15\n",
      "Train Accuracy:  85.9% \tTrain Loss:  0.4337\n",
      "Test Accuracy:   60.3% \tTest Loss:  0.6723\n",
      "EPOCH 105:\tlr: 1.01e-15\n",
      "Train Accuracy:  86.1% \tTrain Loss:  0.4340\n",
      "Test Accuracy:   60.3% \tTest Loss:  0.6723\n",
      "EPOCH 106:\tlr: 7.61e-16\n",
      "Train Accuracy:  86.0% \tTrain Loss:  0.4343\n",
      "Test Accuracy:   60.3% \tTest Loss:  0.6723\n",
      "EPOCH 107:\tlr: 5.71e-16\n",
      "Train Accuracy:  85.8% \tTrain Loss:  0.4335\n",
      "Test Accuracy:   60.3% \tTest Loss:  0.6723\n",
      "EPOCH 108:\tlr: 4.28e-16\n",
      "Train Accuracy:  86.5% \tTrain Loss:  0.4295\n",
      "Test Accuracy:   60.3% \tTest Loss:  0.6723\n",
      "EPOCH 109:\tlr: 3.21e-16\n",
      "Train Accuracy:  85.9% \tTrain Loss:  0.4319\n",
      "Test Accuracy:   60.3% \tTest Loss:  0.6723\n",
      "EPOCH 110:\tlr: 2.41e-16\n",
      "Train Accuracy:  85.9% \tTrain Loss:  0.4310\n",
      "Test Accuracy:   60.3% \tTest Loss:  0.6723\n",
      "EPOCH 111:\tlr: 1.81e-16\n",
      "Train Accuracy:  85.9% \tTrain Loss:  0.4328\n",
      "Test Accuracy:   60.3% \tTest Loss:  0.6723\n",
      "EPOCH 112:\tlr: 1.35e-16\n",
      "Train Accuracy:  86.2% \tTrain Loss:  0.4312\n",
      "Test Accuracy:   60.3% \tTest Loss:  0.6723\n",
      "EPOCH 113:\tlr: 1.02e-16\n",
      "Train Accuracy:  85.5% \tTrain Loss:  0.4336\n",
      "Test Accuracy:   60.3% \tTest Loss:  0.6723\n",
      "EPOCH 114:\tlr: 7.62e-17\n",
      "Train Accuracy:  85.8% \tTrain Loss:  0.4333\n",
      "Test Accuracy:   60.3% \tTest Loss:  0.6723\n",
      "EPOCH 115:\tlr: 5.71e-17\n",
      "Train Accuracy:  85.8% \tTrain Loss:  0.4340\n",
      "Test Accuracy:   60.3% \tTest Loss:  0.6723\n",
      "EPOCH 116:\tlr: 4.29e-17\n",
      "Train Accuracy:  86.0% \tTrain Loss:  0.4316\n",
      "Test Accuracy:   60.3% \tTest Loss:  0.6723\n",
      "EPOCH 117:\tlr: 3.21e-17\n",
      "Train Accuracy:  85.5% \tTrain Loss:  0.4332\n",
      "Test Accuracy:   60.3% \tTest Loss:  0.6723\n",
      "EPOCH 118:\tlr: 2.41e-17\n",
      "Train Accuracy:  85.7% \tTrain Loss:  0.4330\n",
      "Test Accuracy:   60.3% \tTest Loss:  0.6723\n",
      "EPOCH 119:\tlr: 1.81e-17\n",
      "Train Accuracy:  86.0% \tTrain Loss:  0.4315\n",
      "Test Accuracy:   60.3% \tTest Loss:  0.6723\n",
      "EPOCH 120:\tlr: 1.36e-17\n",
      "Train Accuracy:  85.7% \tTrain Loss:  0.4312\n",
      "Test Accuracy:   60.3% \tTest Loss:  0.6723\n",
      "EPOCH 121:\tlr: 1.02e-17\n",
      "Train Accuracy:  85.8% \tTrain Loss:  0.4328\n",
      "Test Accuracy:   60.3% \tTest Loss:  0.6723\n",
      "EPOCH 122:\tlr: 7.63e-18\n",
      "Train Accuracy:  86.0% \tTrain Loss:  0.4330\n",
      "Test Accuracy:   60.3% \tTest Loss:  0.6723\n",
      "EPOCH 123:\tlr: 5.72e-18\n",
      "Train Accuracy:  85.9% \tTrain Loss:  0.4322\n",
      "Test Accuracy:   60.3% \tTest Loss:  0.6723\n",
      "EPOCH 124:\tlr: 4.29e-18\n",
      "Train Accuracy:  86.3% \tTrain Loss:  0.4307\n",
      "Test Accuracy:   60.3% \tTest Loss:  0.6723\n",
      "EPOCH 125:\tlr: 3.22e-18\n",
      "Train Accuracy:  85.9% \tTrain Loss:  0.4329\n",
      "Test Accuracy:   60.3% \tTest Loss:  0.6723\n",
      "EPOCH 126:\tlr: 2.41e-18\n",
      "Train Accuracy:  85.7% \tTrain Loss:  0.4340\n",
      "Test Accuracy:   60.3% \tTest Loss:  0.6723\n",
      "EPOCH 127:\tlr: 1.81e-18\n",
      "Train Accuracy:  85.9% \tTrain Loss:  0.4333\n",
      "Test Accuracy:   60.3% \tTest Loss:  0.6723\n",
      "EPOCH 128:\tlr: 1.36e-18\n",
      "Train Accuracy:  85.8% \tTrain Loss:  0.4339\n",
      "Test Accuracy:   60.3% \tTest Loss:  0.6723\n",
      "EPOCH 129:\tlr: 1.02e-18\n",
      "Train Accuracy:  86.4% \tTrain Loss:  0.4311\n",
      "Test Accuracy:   60.3% \tTest Loss:  0.6723\n",
      "EPOCH 130:\tlr: 7.64e-19\n",
      "Train Accuracy:  86.1% \tTrain Loss:  0.4316\n",
      "Test Accuracy:   60.3% \tTest Loss:  0.6723\n",
      "EPOCH 131:\tlr: 5.73e-19\n",
      "Train Accuracy:  86.0% \tTrain Loss:  0.4306\n",
      "Test Accuracy:   60.3% \tTest Loss:  0.6723\n",
      "EPOCH 132:\tlr: 4.3e-19\n",
      "Train Accuracy:  86.1% \tTrain Loss:  0.4281\n",
      "Test Accuracy:   60.3% \tTest Loss:  0.6723\n",
      "EPOCH 133:\tlr: 3.22e-19\n",
      "Train Accuracy:  86.1% \tTrain Loss:  0.4318\n",
      "Test Accuracy:   60.3% \tTest Loss:  0.6723\n",
      "EPOCH 134:\tlr: 2.42e-19\n",
      "Train Accuracy:  85.9% \tTrain Loss:  0.4359\n",
      "Test Accuracy:   60.3% \tTest Loss:  0.6723\n",
      "EPOCH 135:\tlr: 1.81e-19\n",
      "Train Accuracy:  85.8% \tTrain Loss:  0.4328\n",
      "Test Accuracy:   60.3% \tTest Loss:  0.6723\n",
      "EPOCH 136:\tlr: 1.36e-19\n",
      "Train Accuracy:  85.5% \tTrain Loss:  0.4331\n",
      "Test Accuracy:   60.3% \tTest Loss:  0.6723\n",
      "EPOCH 137:\tlr: 1.02e-19\n",
      "Train Accuracy:  85.7% \tTrain Loss:  0.4355\n",
      "Test Accuracy:   60.3% \tTest Loss:  0.6723\n",
      "EPOCH 138:\tlr: 7.65e-20\n",
      "Train Accuracy:  85.9% \tTrain Loss:  0.4325\n",
      "Test Accuracy:   60.3% \tTest Loss:  0.6723\n",
      "EPOCH 139:\tlr: 5.73e-20\n",
      "Train Accuracy:  86.1% \tTrain Loss:  0.4340\n",
      "Test Accuracy:   60.3% \tTest Loss:  0.6723\n",
      "EPOCH 140:\tlr: 4.3e-20\n",
      "Train Accuracy:  86.1% \tTrain Loss:  0.4326\n",
      "Test Accuracy:   60.3% \tTest Loss:  0.6723\n",
      "EPOCH 141:\tlr: 3.23e-20\n",
      "Train Accuracy:  85.4% \tTrain Loss:  0.4342\n",
      "Test Accuracy:   60.3% \tTest Loss:  0.6723\n",
      "EPOCH 142:\tlr: 2.42e-20\n",
      "Train Accuracy:  86.1% \tTrain Loss:  0.4338\n",
      "Test Accuracy:   60.3% \tTest Loss:  0.6723\n",
      "EPOCH 143:\tlr: 1.81e-20\n",
      "Train Accuracy:  86.3% \tTrain Loss:  0.4317\n",
      "Test Accuracy:   60.3% \tTest Loss:  0.6723\n",
      "EPOCH 144:\tlr: 1.36e-20\n",
      "Train Accuracy:  86.1% \tTrain Loss:  0.4331\n",
      "Test Accuracy:   60.3% \tTest Loss:  0.6723\n",
      "EPOCH 145:\tlr: 1.02e-20\n",
      "Train Accuracy:  85.7% \tTrain Loss:  0.4342\n",
      "Test Accuracy:   60.3% \tTest Loss:  0.6723\n",
      "EPOCH 146:\tlr: 7.65e-21\n",
      "Train Accuracy:  85.8% \tTrain Loss:  0.4332\n",
      "Test Accuracy:   60.3% \tTest Loss:  0.6723\n",
      "EPOCH 147:\tlr: 5.74e-21\n",
      "Train Accuracy:  86.1% \tTrain Loss:  0.4323\n",
      "Test Accuracy:   60.3% \tTest Loss:  0.6723\n",
      "EPOCH 148:\tlr: 4.31e-21\n",
      "Train Accuracy:  86.0% \tTrain Loss:  0.4312\n",
      "Test Accuracy:   60.3% \tTest Loss:  0.6723\n",
      "EPOCH 149:\tlr: 3.23e-21\n",
      "Train Accuracy:  86.1% \tTrain Loss:  0.4301\n",
      "Test Accuracy:   60.3% \tTest Loss:  0.6723\n",
      "EPOCH 150:\tlr: 2.42e-21\n",
      "Train Accuracy:  85.8% \tTrain Loss:  0.4330\n",
      "Test Accuracy:   60.3% \tTest Loss:  0.6723\n"
     ]
    }
   ],
   "source": [
    "model, training_record = train_model(\n",
    "    {\n",
    "        'lr': 1e-2,\n",
    "        'gamma': .75,\n",
    "        'input_size': 2500,\n",
    "        'repeat_layers': 1\n",
    "    }, \n",
    "    150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "3f6baf7e-9056-42ca-b4aa-4ee39e0fdda4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='Epoch', ylabel='Loss'>"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "record_plot(training_record, figsize=(15, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3104280b-5597-455f-bf6c-8e588cc87b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model(model, \"../afib_detector_cnn.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0097178-7b5e-41b3-807c-c7cbb2a0bb7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(\n",
    "    data=[(trial.value, trial.params) for trial in study.trials],\n",
    "    columns=['acc', 'params']\n",
    ").sort_values('acc', ascending=False).to_pickle('../data/cleaned/hyperparameter_optimization.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d6ccbb3c-d270-4f10-9b47-f5f38e3b9a5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Study statistics: \n",
      "  Number of finished trials:  100\n",
      "  Number of pruned trials:  59\n",
      "  Number of complete trials:  41\n",
      "Best trial:\n",
      "  Value:  0.16846804809570312\n",
      "  Params: \n",
      "    gamma: 0.44752485496457467\n",
      "    lr: 0.8531623835455004\n",
      "    repeat_layers: 4\n"
     ]
    }
   ],
   "source": [
    "print(\"Study statistics: \")\n",
    "print(\"  Number of finished trials: \", len(study.trials))\n",
    "print(\"  Number of pruned trials: \", len(pruned_trials))\n",
    "print(\"  Number of complete trials: \", len(complete_trials))\n",
    "\n",
    "print(\"Best trial:\")\n",
    "best_trial = study.best_trial\n",
    "\n",
    "print(\"  Value: \", best_trial.value)\n",
    "\n",
    "print(\"  Params: \")\n",
    "for key, value in best_trial.params.items():\n",
    "    print(\"    {}: {}\".format(key, value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce9fea45-0ba9-462a-9ad2-3b50bdcf6033",
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
